# Methodology {#methodology}

In conventional Vector Autoregression (VAR) dependencies of any system variable on past realizations of itself and its covariates are modelled through linear equations. This corresponds to a particular case of the broader class of Deep Vector Autoregressions investigated here and will serve as the baseline for our analysis.

## Vector Autoregression {#var}

Let $\mathbf{y}_t$ denote the $(K \times 1)$ vector of variables at time $t$. Then the VAR($p$) with $p$ lags and a constant deterministic term is simply a linear system of stochastic equations of the following form:

\begin{equation} 
\begin{aligned}
&& \mathbf{y}_t&=\mathbf{c} + \mathbf{A}_1 \mathbf{y}_{t-1} + \mathbf{A}_2 \mathbf{y}_{t-2} + ... + \mathbf{A}_p \mathbf{y}_{t-p} + \mathbf{u}_t \\
\end{aligned}
(\#eq:red-form)
\end{equation}

The matrices $\mathbf{A}_m$, $m\in\{1,...,p\}$ contain the reduced form coefficients and $\mathbf{u}_t$ is a vector of errors for which $\mathbb{E}\mathbf{u}_t$, $\mathbb{E}\mathbf{u}_t\mathbf{u}_t^T=\Sigma$ and $\mathbb{E}\mathbf{u}_t\mathbf{u}_s^T=\mathbf{0}$ for all $t\ne s$. We refer to \@ref(eq:red-form) as the *reduced form* representation of the VAR($p$) because all right-hand side variables are predetermined [@kilian2017structural]. 

To facilitate the discussion of Deep VARs below it is helpful to be explicit about how each individual time series is modelled. In particular, it follows from \@ref(eq:red-form) that 

$$
\begin{aligned}
&& y_{it}&=c_i+ \sum_{m=1}^{p} \sum_{j=1}^{K}a_{jm}y_{jt-m}+u_{it} \\
\end{aligned}
$$

which corresponds to the key modelling assumption that at any point in time $t$ any time series $i\in1,...,K$ is just a weighted sum of past realizations of itself and all other variables in the system. This assumption makes the estimation VAR($p$) processes remarkably simple. Perhaps more importantly, the assumption of linearity also greatly facilitates inference about VARs.

For implementation purposes and in order to state stationarity conditions it is convenient to restate the $K$-dimensional VAR($p$) process more compactly in companion form as

\begin{equation} 
\begin{aligned}
&& \mathbf{Y}_t&= \begin{pmatrix}
 \mathbf{c} \\
0 \\
\vdots \\
0 \\
\end{pmatrix} + \mathbf{A} \mathbf{Y}_{t-1} + \begin{pmatrix}
 \mathbf{u}_t \\
0 \\
\vdots \\
0 \\
\end{pmatrix}  \\
\end{aligned}
(\#eq:companion)
\end{equation}

where $\mathbf{Y}_t=(\mathbf{y}_t^T,...,\mathbf{y}^T_{t-p+1})^T$ and $\mathbf{A}$ is referred to as the companion matrix [@kilian2017structural]. Letting $\mathbf{Z}=(\mathbf{1}, \mathbf{Y})$ we have a simple closed form solution for estimating reduced form coefficients through ordinary least squares (OLS):

\begin{equation} 
\begin{aligned}
&& \widehat{
\left(\begin{pmatrix}
 \mathbf{c} \\
0 \\
\vdots \\
0 \\
\end{pmatrix},\mathbf{A}\right)
}&= \left( \mathbf{Z}_{t-1}^T\mathbf{Z}_{t-1} \right)^{-1} \mathbf{Z}_{t-1}^T \mathbf{Y}_t\\
\end{aligned}
(\#eq:ols)
\end{equation}

## Model selection

The simplicity that characterizes the linear VAR model 

### Stationarity {#stationarity}

When working with time series we are generally concerned about stationarity. Broadly speaking stationarity ensures that the future is like the past and hence any predictions we make based on past data adequately describe future outcomes. In the context of VARs stationarity is follows from stability: a VAR($p$) is stable if the effects of shocks to the system eventually die out. Stability can assessed through the system's autoregressive roots or equivalently by looking at the eigenvalues of companion matrix $\mathbf{A}$ [@kilian2017structural]. In particular, for the VAR($p$) in \@ref(eq:companion) to be stable we condition that the $Kp$ eigenvalues $\lambda$ that satisfy 

$$
\begin{aligned}
&& \det (\mathbf{A} - \lambda \mathbf{I}_{Kp})&=0 \\
\end{aligned}
$$

are all of absolute value less than one. Stability implies that the first and second moments of the VAR($p$) process are time-invariant, hence ensuring weak stationarity [@kilian2017structural]. 

A straight-forward way to deal with stationarity of VARs is to simply ensure that the individual time series entering the system are stationary. This usually involves differencing the time series until they are stationary: for any time series $y_i$ that is integrated of order $I(\delta)$, there exists a $\delta$-order difference that is stationary. An immediate drawback of this approach is the loss of information contained in the levels of the time series. Modelling approaches that take into account conintegration of individual time series can ensure system stationarity and still let individually non-stationary time series enter the system in levels [@hamilton1994time]. 

### Lag order

## Deep Vector Autoregression {#deepvar}






