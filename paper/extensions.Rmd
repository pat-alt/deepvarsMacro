# Caveats and extensions {#exten}

In this work we have provided empirical evidence that the introduction of deep learning can lead to improved modelling and forecasiting performance in the context of macroeconomic time series data. While we believe that our proposed methodology extends the conventional VAR framework quite naturally, it still comes with a lot of added complexity. Unfortunately, in the case of deep learning this added complexity also entails reduced interpretability: even though we have intentionally worked with a relatively small and simple neural network architecture, the number of parameters and interactions between neurons that they govern cannot possibly be interpreted by a human. This is why deep artificial neural networks are commonly referred to as black boxes. 

Perhaps more importantly in the context of time series forecasting, it is also much harder to quantify predictive uncertainty of deep neural networks: while confidence intervals around point forecasts from a linear VAR can be computed using closed-form analytical expressions [@kilian2017structural], no such expressions exist in the context of Deep VAR. Future work on this issue will most likely rely on probabilistic deep learning, which has gained popularity in recent years. Among the most widely used approaches to uncertainty quantification for deep learning are deep ensembles [@lakshminarayanan2016simple] and Monte Carlo dropout [@gal2016dropout]. The former boils down to training not just one but multiple networks and effectively averaging over predictions: since weights are initialized randomly, predictions are stochastic. The latter similarly introduces stochasticity by activating dropout not only during training but also at the testing stage. A common drawback of these and other approaches that rely on Monte Carlo is the increased computational burden. As an alternative to Monte Carlo @daxberger2021laplace have recently shown that Laplace approximation can be used for effortless Bayesian deep learning.

Support for the estimation of impulse response functions is another missing cornerstone in the current version of our proposed framework. IRFs are used to understand how system variables change in response to unit shocks to any of the system variables. When estimating the model with the traditional VAR, IRFs can be readily derived from the reduced form model coefficients. Generalized (or structural) IRFs require the system to be fully identified, which is typically achieved through restrictions on contemporaneous (and likely correlated) reduced-form errors. In the context of Deep VAR further research is required concerning both computation of IRFs and the identification problem. @verstyuk2020modeling computes impulse response functions for their proposed MLSTM numerically and relies on a Cholesky decomposition of the reduced-form covariance matrix, just like in the conventional setting. A more desirable approach may once again involve probabilistic deep learning: @ish2019interpreting proposes a straight-forward approach towards producing global feature importance measures for input features of Bayesian neural networks. It might be possible to leverage these importance measures as proxies for the conventional VAR's linear coefficients and produce approximate impulse response functions for Deep VAR models in the same way as for conventional VAR models. Of course, these are merely rough ideas for future research.

While we very much recognize the need for model interpretability especially in the context of policy-making, we believe that the Deep VAR framework proposed here can be augmented to meet these demands. Considerable further effort will likely be required to this end. With respect to the main question posed at the beginning of this work, we conclude that deep learning may be leveraged effectively in the context of macroeconomic time series modelling.

