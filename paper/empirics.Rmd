# Empirical results {#empirics}

```{r, include=FALSE}
knitr::opts_chunk$set(
  eval=FALSE,
  echo = FALSE,
  warning=FALSE,
  message=FALSE
)
library(deepvars)
library(ggplot2)
```

```{r, eval=TRUE}
dt <- readRDS("../data/data_VAR/preprocessed.rds")
dt_l <- data.table::melt(dt, id.vars="date")
data.table::setkey(dt_l, date, variable)
var_cols <- colnames(dt)[2:ncol(dt)]
```

```{r, eval=TRUE}
# Choosing lags:
max_lags <- 12
lags <- lag_order(dt, max_lag = max_lags)$p
# Deep VAR params:
num_units <- 120
num_layers <- formals(deepvareg)$num_layers
dropout <- formals(deepvareg)$p_drop_out
epochs <- 1200
```

```{r, eval=FALSE}
# VAR fitting
var_model <- vareg(dt, lags = lags)
# Deep VAR fitting
deepvar_model <- deepvareg(
  dt, 
  lags=lags, 
  num_units=num_units, 
  epochs=epochs
)
# Threshold VAR
tv <- tsDyn::TVAR(dt[,-1], include = "const", lag=lags, nthresh=2, trim=0.1)
tv_l <- data.table::melt(data.table::data.table(date=dt$date[-(1:lags)], fitted(tv)), id.vars = "date", value.name = "y_hat")
data.table::setkey(tv_l, date, variable)
tv_l <- dt_l[tv_l]
# Random forest
rf <- data.table::fread("../results/predictions_RF.csv")
rf[,date:=var_model$model_data$data[(var_model$model_data$lags+1):.N]$date]
rf_l <- data.table::melt(rf, id.vars="date", value.name = "y_hat")
data.table::setkey(rf_l, date, variable)
rf_l <- dt_l[rf_l]
```

```{r, eval=FALSE}
# Cum RMSE plot
cum_loss_var <- cum_loss(var_model)$cum_loss[,type:="VAR"]
cum_loss_dvar <- cum_loss(deepvar_model)$cum_loss[,type:="Deep VAR"]
cum_loss_tv <- tv_l[,value:=cumsum((value-y_hat)^2),by=.(variable)][,.(date, variable, value)][,type:="TVAR"]
cum_loss_rf <- rf_l[,value:=cumsum((value-y_hat)^2),by=.(variable)][,.(date, variable, value)][,type:="RF"]
cum_loss_rw <- dt_l[
  ,
  .(
    date=date,
    value=cumsum((value-data.table::shift(value,fill=0))^2),
    type="Random walk"
  ),
  by=variable
]
dt_plot <- rbind(cum_loss_dvar, cum_loss_var, cum_loss_rw, cum_loss_tv, cum_loss_rf)
p <- ggplot2::ggplot(data=dt_plot, ggplot2::aes(x=date, y=value, colour=type)) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(~variable, scales = "free_y") +
  ggplot2::scale_color_discrete(name="Model:") +
  ggplot2::labs(
      x="Date",
      y="Squared error"
    ) +
  ggplot2::geom_hline(ggplot2::aes(yintercept=0), lwd=0.1)
ggplot2::ggsave(
  filename = "../www/cum_loss_full.png", 
  plot = p,
  width = 6,
  height = 4
)
```

```{r, eval=FALSE}
resids <- rbind(
  deepvar_model$res[,model:="Deep VAR"],
  data.table::data.table(var_model$res)[,model:="VAR"]
)
resids <- data.table::melt(resids, id.vars = "model")
p <- ggplot(data=resids, aes(sample=value)) +
  stat_qq() + stat_qq_line() +
  facet_wrap(model~variable, scales="free_y", nrow=2) +
  labs(
    x="Theoretical",
    y="Sample"
  )
ggsave(
  filename = "../www/qqplot_full.png", 
  plot = p,
  width = 8,
  height = 4
)
```

```{r, eval=FALSE}
library(data.table)
lag_max <- 25
ci <- .95
resids_acf <- resids[
  ,
  .(
    acf=stats::acf(value, lag.max = lag_max, plot=FALSE)$acf[,,1], 
    lag=stats::acf(value, lag.max = lag_max, plot=FALSE)$lag[,,1],
    ci=qnorm((1 + ci)/2)/sqrt(.N)
  ),
  by=.(model, variable)
]
p <- ggplot(data=resids_acf, aes(x=lag, y=acf)) +
  geom_hline(aes(yintercept = ci), linetype = "dashed", color = "darkblue") +
  geom_hline(aes(yintercept = 0)) +
  geom_hline(aes(yintercept = -ci), linetype = "dashed", color = "darkblue") +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  facet_wrap(model~variable, scales="free_y", nrow=2) +
  labs(
    y="ACF",
    x="Lag"
  ) 
ggsave(
  filename = "../www/acf_full.png", 
  plot = p,
  width = 8,
  height = 4
)
```

We now proceed to benchmark the proposed Deep VAR model against the conventional VAR using out macroeconomic time series data. To begin with, we compare both models in terms of their in-sample fit. For this part of the analysis the models will be strictly run under the same framing conditions. Due to the RNN's capacity to essentially model any possible function $f_i(\cdot)$ the Deep VAR dominates the VAR in this realm. We investigate during what time periods the outperformance of the Deep VAR is particularly striking to gain a better understanding of when and why it pays off to relax the linearity constraint.

These findings with respect to in-sample performance provide some initial evidence in favor of the Deep VAR. But since a reduction in modelling bias is typically associated with an increase in variance, we are particularly interested in benchmarking the models with respect to their out-of-sample performance. To this end we split our sample into train and test subsamples. We then firstly benchmark the models in terms of their pseudo out-of-sample fit. Finally we also look at model performance with respect to $n$-step ahead pseudo out-of-sample forecasts.

The final part of this section relaxes the constraint on the framing conditions. In particular, we investigate how hyperparameter tuning with respect to the neural network architecture and lag length $p$ can improve the performance of the Deep VAR.

## In-sample fit

For this first empirical exercise both models are trained on the full sample. We have decided to include the post-Covid sample period despite the associated structural break, since it serves as interesting point of comparison. The optimal lag order as determined by the Akaike Information Criterium is $p=`r lags`$, where we used a maximum possible lag of $p_{\max}=`r max_lags`$ corresponding to one year. A look at the eigenvalues of the companion matrix showed that the VAR($`r lags`$) is stable. The LSTMs underlying the Deep VAR model are composed of $H=`r num_layers`$ that count $N=`r num_units`$ hidden units each. The dropout rate is set to $p=`r dropout`$. 

To assess the fit of our models we use the root mean squared error (RMSE) as our preferred loss function. Figure \@ref(fig:cum-loss-full) shows the cumulative RMSE of the Deep VAR model and its conventional benchmarks for each of the time series over the whole sample period. Aside from the linear VAR, we have also added another popular approach towards VAR models that addresses non-linearity (Threshold VAR). The first thing we can observe is that the RMSE of the Deep VAR is consistently flatter than the RMSE of its benchmarks. With respect to in-sample 1-step ahead predictions, the Deep VAR dominates throughout the almost the entire sample period and for all of the considered variables. This empirical observation seems to confirm our expectation that the vector autoregressive process is characterized by important non-linear dependencies across time and variables that the conventional VAR and even the TVAR fail to capture. 

```{r cum-loss-full, eval=TRUE, fig.cap="Comparison of cumulative loss over the entire sample period for Deep VAR and benchmarks."}
knitr::include_graphics("../www/cum_loss_full.png")
```

Figure \@ref(fig:cum-loss-full) is especially useful to asses in which specific periods the Deep VAR model achieves better predictive performance than the VAR model. From the very beginning and across variables, we observe that the increase in cumulative loss for the VAR model is greater than for the Deep VAR model. The US economy during 1960s was influence by John F. Kennedy's introduction of **New Economics**, which was informed by Keynesian ideas and characterized by increasing levels of inflation, a reduction in unemployment and output growth. The change in government certainly corresponded to a regime switch with respect to the economy [@perry2010economic] and in that sense it is interesting to observe that the Deep VAR appears to be doing a better job at capturing the underlying changes. The 1970s can be broadly thought of as a continuation of New Economics and loosely defined as a period of stagflation. The Deep VAR continues to outperform the VAR during that period.

The first truly interesting development we can observe in Figure \@ref(fig:cum-loss-full) coincides with the onset of the Volcker disinflation period. Following years of sustained CPI growth, Paul Volcker set the Federal Reserve on course for a series of interest rate hikes as soon as he became chairperson of the central bank in August 1979. The shift in monetary policy triggered fundamental changes to the US economy and in particular the key economic indicators we are analyzing here throughout the 1980s [@goodfriend2005incredible]. Despite this structural break, the increase in the cumulative RMSE of the Deep VAR remains almost constant during this decade for most variables. The performance of the VAR on the other hand is unsurprisingly poor over the same period, in particularly so for the CPI and the Fed Funds Rate, which arguably were the two variables most directly affected by the change in policy. The Deep VAR also clearly dominates the VAR with respect to the output related variables (IP) and to a lesser extent unemployment. These findings indicate that changes to the monetary transmission mechanism in response to sudden policy shifts are not well captured by a linear-additive vector autoregressive model. Instead they appear to unfold in a high-dimensional latent state space, which the Deep VAR by its very construction is designed to learn.

Following the Volcker disinflation period, Figure \@ref(fig:cum-loss-full) does not reveal any clear outperformance of either of the models during the 1990s. Interestingly the dot-com bubble has little affect on either of the models, aside from a small pick-up in cumulative loss with respect to the CPI for both models. With all that noted, the Deep VAR still continuously outperforms the VAR since evidently its cumulative loss increases at a lower pace altoghether. 

As the Global Financial Crisis unfolds around 2007 the pattern we observed for the Volcker disinflation reemerges, albeit to a lesser extent: there is a marked jump in the difference between the cumulative loss of the VAR and the Deep VAR, in particular so for the CPI, the Fed Funds rate and industrial production. The gap for all these variables continues to widen during the aftermath of the crisis. The Deep VAR once again does a better job at modelling the changes that the dynamical system undergoes: post-crisis US monetary policy was characterized by very low interest rates, low levels of inflation as well as the introduction of a range of non-conventional monetary policy tools including quantitative easing and forward guidance. 

Finally, it is also interesting to observe how both models perform in response to the  unprecedented exogenous shock that Covid-19 constitutes. Both models incur huge errors with respect to both IP and UR - the two series most significantly affected by Covid. Evidently though, the magnitude of the errors is somewhat larger for the VAR than for the Deep VAR. This, once again seems to confirm our hypothesis that the Deep VAR model captures important non-linear dependencies across time and variables that the conventional VAR fails to capture.

As a sanity check we also visually inspected the distributional properties of the model residuals for the full-sample fit. The outcomes are broadly consistent across models: while for some variables residuals are clearly not Gaussian, we see no evidence of serial autocorrelation of residuals (see Figures \@ref(fig:qqplot-full) and \@ref(fig:acf-full) in the appendix). 

\FloatBarrier

## Out-of-sample fit {#oosample}

```{r, eval=TRUE}
#Train test split
train_test_split <- split_sample(dt)
train_data <- train_test_split$train_data
#Select the number of lags
lags <- lag_order(train_data, max_lag = max_lags)$p
```

```{r, eval=FALSE}
# VAR
var_model <- vareg(train_data, lags = lags)
# Deep VAR
deepvar_model <- deepvareg(
  train_data, 
  lags=lags, 
  num_units = num_units, 
  epochs=epochs
)
```

```{r, eval=FALSE}
y_true <- var_model$y_train
```

In order to assess if the Deep VAR's outperformance is a consequence of overfitting, we now repeat the previous exercise, but this time we train the models on a subsample of our date. The training sample spans from `r format(min(train_data$date), "%B, %Y")` to `r format(max(train_data$date), "%B, %Y")`, whereas the test data goes from `r format(min(train_test_split$test_data$date), "%B, %Y")` to `r format(max(train_test_split$test_data$date), "%B, %Y")`. This corresponds to training the model on 80 percent of the data and retaining the remaining 20 percent for testing purposes. The optimal lag order for the training subsample is $p=`r lags`$ where we use the same criterion and maximum lag order as before. Once again we find this VAR specification to be stable.

```{r, eval=FALSE}
#VAR
pred_var <- predict(var_model)
p <- plot(pred_var, y_true = y_true)
ggsave("../www/pred_var_train.png", width = 7, height = 4)
#DeepVAR
pred_deepvar <- predict(deepvar_model)
p <- plot(pred_deepvar, y_true = y_true)
ggsave("../www/pred_dvar_train.png", width = 7, height = 4)
```

```{r, eval=FALSE}
#Predictions out of sample
X_test <- prepare_test_data(train_test_split, lags=lags)$X_test
y_test <- prepare_test_data(train_test_split, lags=lags)$y_test
#VAR
pred_var <- predict(var_model, X=X_test)
p <- plot(pred_var, y_true = y_test)
ggsave("../www/pred_var_test.png", width = 7, height = 4)
#DeepVAR
pred_dvar <- predict(deepvar_model, X=X_test)
p <- plot(pred_dvar, y_true = y_test)
ggsave("../www/pred_dvar_test.png", width = 7, height = 4)
```

```{r, eval=FALSE}
#RMSE 
rmse_var <- rbind(
  rmse(var_model)[,model:="var"][,sample:="train"],
  rmse(var_model, X=X_test, y=y_test)[,model:="var"][,sample:="test"]
)
rmse_dvar <- rbind(
  rmse(deepvar_model)[,model:="deepvar"][,sample:="train"],
  rmse(deepvar_model, X=X_test, y=y_test)[,model:="deepvar"][,sample:="test"]
)
tab_rmse <- rbind(rmse_var, rmse_dvar)
tab_rmse <- data.table::dcast(tab_rmse, sample + variable ~ model, value.var = "value")
tab_rmse[, ratio:= deepvar/var]
saveRDS(tab_rmse, file="../results/train_test_tab_rmse.rds")
```

Tables \@ref(tab:rmse) shows the Root Mean Squared Error (RMSE) for the in-sample and the out-of-sample predictions of both the VAR model and the Deep VAR model. We can see that the RMSE for the Deep VAR outperforms the one for the conventional VAR for both the training data and the test data and for all time series. The fifth column of the table shows us the ratio between the RMSEs of the Deep VAR and the VAR: the lower the ratio, the better the Deep VAR compared to the VAR. With respect to the training sample, the RMSE of the Deep VAR model is consistently less than 75% of that of the conventional VAR reflecting to some extent the results of the previous sections. Turning to the test data, there is no evidence that the Deep VAR is more prone to overfitting than the VAR. For both industrial production and unemployment, the Deep VAR yields an RMSE that is around half the size of that produced by the VAR. For inflation and interest rate predictions the outperformance on the test data is less striking, but still fairly large.

```{r rmse, eval=TRUE}
tab_rmse <- readRDS("../results/train_test_tab_rmse.rds")
knitr::kable(
  tab_rmse, 
  col.names = c("Sample", "Variable", "DVAR", "VAR", "Ratio (DVAR / VAR)"),
  digits = 5,
  caption = 'Root mean squared error (RMSE) for the two models across subsamples and variables.'
) 
```


```{r, include=FALSE, eval=TRUE}
library(xtable)
dtf<- xtable(tab_rmse[, -"Mean"], caption = c("RMS"))
names(dtf) <- c('Sample','Variable', 'DeepVAR', "VAR", "Ratio")
bold <- function(x){
paste('{\\textbf{',x,'}}', sep ='')
}
italic <- function(x){
paste0('{\\emph{ ', x, '}}')
}
print(dtf, sanitize.colnames.function = bold, sanitize.rownames.function = italic, booktabs = TRUE, include.rownames=FALSE)
```

## Forecasts

```{r, eval=FALSE}
n_ahead <- 12
y_true <- y_test[1:n_ahead,]
```

```{r, eval=FALSE}
#Forecasts
#VAR
fcst_var <- deepvars::forecast(var_model, n.ahead = n_ahead)
p <- deepvars:::plot.forecast(fcst_var, y_true=y_true, history = 20)
ggsave(filename = "../www/fcst_var.png", p, width=7, height=4)
#DeepVAR
fcst_dvar <- deepvars::forecast(deepvar_model, n.ahead = n_ahead)
p <- deepvars:::plot.forecast(fcst_dvar, y_true=y_true, history = 20)
ggsave(filename = "../www/fcst_dvar.png", p, width=7, height=4)
```

```{r, eval=FALSE}
# Correlations
correlations <- function(fcst_var,fcst_dvar,y_true){
  list_names <- c(colnames(y_true))
  cor_var <- c()
  cor_dvar <- c()
  for (n in list_names){
    cor_var <- c(cor_var,cor(fcst_var$fcst[, ..n][-1], y_true[,n]))
    cor_dvar <- c(cor_dvar,cor(fcst_dvar$fcst[, ..n][-1], y_true[,n]))
    
  }
  return(data.table(cor_var,cor_dvar))
}
correlations <- correlations(fcst_var,fcst_dvar,y_true)

# Table
tab_fcst <- data.table(
  "Variable" = colnames(y_true), 
  "VAR RMSFE"= rmsfe(fcst_var, y_true = y_true)[,value] , 
  "Deep VAR RMSFE"= rmsfe(fcst_dvar, y_true = y_true)[,value],
  "VAR correlations" = correlations$cor_var,
  "Deep-VAR correlations" = correlations$cor_dvar
)
saveRDS(tab_fcst, file="../results/tab_fcst.rds")
```

Up until now we have been assessing the 1-step ahead predictions of both models. In our context these predictions can be thought of as 1-month ahead nowcasts from a practical perspective. Since real-time nowcasts have grown in popularity during recent years, the results so far should be of great interest to central bankers and other practitioners.
Nonetheless, there is typically also great interest in time series forecasts at longer horizons. We therefore briefly introduce $n$-step ahead pseudo out-of-sample forecasts in this section and revisit them again further below.

Forecasts are produced recursively both for the VAR and the Deep VAR. Specifically, we use the models we trained on the training data to recursively predict one time period ahead, concatenate the predictions to the training data and repeat the process.^[Note that for the Deep VAR an alternative approach would be to work with a different output dimension for the underlying neural networks.] This way we produce one-year ahead forecasts beginning from the first date in the test sample (`r format(max(train_data$date), "%B, %Y")`).

Table \@ref(tab:fcst) shows the resulting root mean squared forecast errors (RMSFE) along with correlation between forecasts and realizations. As we can see in the table, the RMSFE of the Deep VAR is consistently lower than the one for the VAR. Regarding correlations the VAR produces forecasts that are negatively correlated with actual outcomes for all time series: in other words, when the time series evolves in one direction, the VAR forecast tends to evolve in the opposite direction. For industrial production, the Deep VAR forecast also has a highly negative correlation with the actual values. For the rest of time series the Deep VAR forecasts correlate positively with actual outcome, albeit weakly. Another general observation we made with respect to these forecasts is that the forecasts from the conventional VAR are fairly volatile, while the Deep VAR forecasts swiftly revert to steady levels (see Figures \@ref(fig:fcst-var) and \@ref(fig:fcst-dvar) in the appendix).

```{r fcst, eval=TRUE}
tab_fcst <- readRDS("../results/tab_fcst.rds")
knitr::kable(
  tab_fcst, digits=5,
  caption = "Comparison of n-step ahead pseudo out-of-sample forecasts."
)
```

```{r, eval=FALSE}
dtf<- xtable(tab_fcst, caption = c("Forecasts results"))
print(dtf, sanitize.colnames.function = bold, sanitize.rownames.function = italic, booktabs = TRUE, include.rownames=FALSE,size = "\\setlength{\\tabcolsep}{2pt}")
```

### Rolling window

```{r roll, eval=FALSE}
source("../R/utils.R")
fcst <- rolling_window_fe(dt,w_size = 240, n_ahead = 12)
saveRDS(fcst, "../results/rolling_fcst.rds")
```

```{r}
fcst <- readRDS("../results/rolling_fcst.rds")
```

```{r}
library(ggplot2)
ggplot(data = fcst[model!="tv",.(error=mean(sqerror)),by=.(variable,model,id)], aes(x=id, y=error, fill=model)) +
  geom_col(position = "dodge") +
  facet_wrap(.~variable, scales = "free") 
```

```{r, fig.width=10, fig.height=8}
library(ggplot2)
plot_data <- fcst[id %in% c(1,3,6,12),.(window=window,error=cumsum(sqerror)),by=.(variable,model,id)]
ggplot(data = plot_data[model!="tv"]) +
  geom_line(aes(x=window, y=error, color=model)) +
  facet_wrap(id ~ variable, scales = "free") 
```


## Varying hyperparameters

```{r, eval=TRUE}
# Parameters
n_ahead <- 12
layers <- c(1,2,5)
units <- c(50,100,150)
dropout <- c(0.3,0.5,0.7)
lags <- c(10,50,100)
grid <- data.table::CJ(
  layer = layers,
  unit = units,
  dropout = dropout
)
n_sim <- length(lags) * grid[,.N]
```

```{r, eval=TRUE}
# Train test:
train_test_split <- split_sample(dt)
train_data <- train_test_split$train_data
```

```{r, eval=FALSE}
grid_search <- rbindlist(
  lapply(
    1:length(lags),
    function(lag_idx) {
      
      p <- lags[lag_idx]
      
      message(sprintf("Running for lag=%i", p))
      
      # For given lag fit VAR:
      var_model <- vareg(train_data, lags = p) 
      y_true <- var_model$y_train
      
      # Test data:
      X_test <- prepare_test_data(train_test_split, lags=p)$X_test
      y_test <- prepare_test_data(train_test_split, lags=p)$y_test
      y_true_fcst <- y_test[1:n_ahead,]
      
      # Enter loop for Deep VAR:
      results <- rbindlist(
        lapply(
          1:nrow(grid),
          function(i) {
            
            list2env(c(grid[i,]), envir = environment()) # retrieve parameters
            
            pct_done <- ( ( (lag_idx-1)*grid[,.N] + i ) / n_sim ) * 100
            message(sprintf("Percent done: %0.2f", pct_done))
            
            # Fit Deep VAR model
            deepvar_model <- deepvareg(
              train_data, 
              lags=p, 
              num_units = unit,
              num_layers = layer,
              epochs=100,    
              p_drop_out = dropout
            )
            
            # Train RMSE
            train_rmse <- rbind(
              rmse(deepvar_model)[,model:="dvar"][,measure:="rmse"][,sample:="train"],
              rmse(var_model)[,model:="var"][,measure:="rmse"][,sample:="train"]
            )
            
            # Test RMSE
            test_rmse <- rbind(
              rmse(
                deepvar_model, X=X_test, y=y_test
              )[,model:="dvar"][,measure:="rmse"][,sample:="test"],
              rmse(
                var_model, X=X_test, y=y_test
              )[,model:="var"][,measure:="rmse"][,sample:="test"]
            )
            
            # Forecasts RMSE
            fcst_dvar <- forecast(deepvar_model, n.ahead = n_ahead)
            fcst_var <- forecast(var_model, n.ahead = n_ahead)
            fcst_rms <- rbind(
              rmsfe(
                fcst_dvar, y_true = y_true_fcst
              )[,model:="dvar"][,measure:="rmsfe"][,sample:="test"],
              rmsfe(
                fcst_var, y_true = y_true_fcst
              )[,model:="var"][,measure:="rmsfe"][,sample:="test"]
            )
            
            # Forecasts correlations:
            fcst_corr <- rbind(
              cor_fcst(
                fcst_dvar, y_true = y_true_fcst
              )[,model:="dvar"][,measure:="corr"][,sample:="test"],
              cor_fcst(
                fcst_var, y_true = y_true_fcst
              )[,model:="var"][,measure:="corr"][,sample:="test"]
            )
            
            # Put together:
            results <- rbind(
              train_rmse, 
              test_rmse, 
              fcst_rms, 
              fcst_corr
            )
            
            # Assign remaining variables:
            results[,lag:=p]
            results[,num_layer:=layer]
            results[,num_units:=unit]
            results[,dropout:=dropout]
            
            return(results)
          }
        )
      )
      
      return(results)
      
    }
  )
)
saveRDS(grid_search, "../results/grid_search.rds")
```

```{r, eval=TRUE}
grid_search <- readRDS("../results/grid_search.rds")
```

```{r}
library(ggpubr)
```

```{r, eval=FALSE}
p_list <- invisible(
  lapply(
    unique(grid_search$variable),
    function(var) {
      p <- ggplot(
        data=grid_search[
          sample=="test" & 
            measure=="rmse" & 
            model =="dvar" & 
            variable == var
        ], 
        aes(x=lag, y=value, colour=factor(dropout))
      ) +
        scale_colour_discrete(name="Dropout rate:") +
        geom_line() + geom_point() +
        facet_grid(
          rows=vars(num_layer),
          cols=vars(num_units)
        ) +
        labs(
          x="Lag",
          y="RMSE",
          title = var
        )
      return(p)
    }
  )
)
p <- ggarrange(
  plotlist = p_list, ncol=2, nrow=2, 
  common.legend = TRUE, legend="bottom"
)
ggsave("../www/grid_rmse_test.png", plot=p, width = 9, height = 9)
```

```{r, eval=FALSE}
p_list <- invisible(
  lapply(
    unique(grid_search$variable),
    function(var) {
      p <- ggplot(
        data=grid_search[
          sample=="train" & 
            measure=="rmse" & 
            model =="dvar" & 
            variable == var
        ], 
        aes(x=lag, y=value, colour=factor(dropout))
      ) +
        scale_colour_discrete(name="Dropout rate:") +
        geom_line() + geom_point() +
        facet_grid(
          rows=vars(num_layer),
          cols=vars(num_units)
        ) +
        labs(
          x="Lag",
          y="RMSE",
          title = var
        )
      return(p)
    }
  )
)
p <- ggarrange(
  plotlist = p_list, ncol=2, nrow=2, 
  common.legend = TRUE, legend="bottom"
)
ggsave("../www/grid_rmse_train.png", plot=p, width = 9, height = 9)
```

```{r, eval=FALSE}
p_list <- invisible(
  lapply(
    unique(grid_search$variable),
    function(var) {
      p <- ggplot(
        data=grid_search[
          sample=="test" & 
            measure=="rmsfe" & 
            model =="dvar" & 
            variable == var
        ], 
        aes(x=lag, y=value, colour=factor(dropout))
      ) +
        scale_colour_discrete(name="Dropout rate:") +
        geom_line() + geom_point() +
        facet_grid(
          rows=vars(num_layer),
          cols=vars(num_units)
        ) +
        labs(
          x="Lag",
          y="RMSFE",
          title = var
        )
      return(p)
    }
  )
)
p <- ggarrange(
  plotlist = p_list, ncol=2, nrow=2, 
  common.legend = TRUE, legend="bottom"
)
ggsave("../www/grid_rmsfe_test.png", plot=p, width = 9, height = 9)
```

```{r, eval=FALSE}
p_list <- invisible(
  lapply(
    unique(grid_search$variable),
    function(var) {
      p <- ggplot(
        data=grid_search[
          sample=="test" & 
            measure=="rmse" & 
            dropout == 0.5 & 
            variable == var
        ], 
        aes(x=lag, y=value, colour=factor(model))
      ) +
        scale_colour_discrete(name="Model:") +
        geom_line() + geom_point() +
        facet_grid(
          rows=vars(num_layer),
          cols=vars(num_units)
        ) +
        labs(
          x="Lag",
          y="RMSFE",
          title = var
        )
      return(p)
    }
  )
)
p <- ggarrange(
  plotlist = p_list, ncol=2, nrow=2, 
  common.legend = TRUE, legend="bottom"
)
ggsave("../www/grid_rmse_benchmark.png", plot=p, width = 9, height = 9)
```

```{r, eval=FALSE}
p_list <- invisible(
  lapply(
    unique(grid_search$variable),
    function(var) {
      p <- ggplot(
        data=grid_search[
          sample=="test" & 
            measure=="rmsfe" & 
            dropout == 0.5 & 
            variable == var
        ], 
        aes(x=lag, y=value, colour=factor(model))
      ) +
        scale_colour_discrete(name="Model:") +
        geom_line() + geom_point() +
        facet_grid(
          rows=vars(num_layer),
          cols=vars(num_units)
        ) +
        labs(
          x="Lag",
          y="RMSFE",
          title = var
        )
      return(p)
    }
  )
)
p <- ggarrange(
  plotlist = p_list, ncol=2, nrow=2, 
  common.legend = TRUE, legend="bottom"
)
ggsave("../www/grid_rmsfe_benchmark.png", plot=p, width = 9, height = 9)
```

```{r, eval=FALSE}
dt_plot <- grid_search[
  sample=="test" &
    measure %in% c("rmse", "rmsfe") &
    dropout == 0.5 & 
    num_units == 50 & 
    num_layer == 1
][,.(variable, value, model, measure, lag)][,type:="Grid search"]

# Merge in results from initial run:
tab_rmse <- readRDS("../results/train_test_tab_rmse.rds")
tab_rmse <- tab_rmse[sample=="test"][, sample:=NULL]
tab_rmse[,ratio:=NULL]
setnames(tab_rmse, "deepvar", "dvar")
tab_rmse <- melt(tab_rmse, id.vars = "variable", variable.name = "model")
tab_rmse[,measure:="rmse"]
tab_fcst <- readRDS("../results/tab_fcst.rds")
tab_fcst[,`VAR correlations`:=NULL][,`Deep-VAR correlations`:=NULL]
setnames(tab_fcst, colnames(tab_fcst), c("variable", "var", "dvar"))
tab_fcst <- melt(tab_fcst, id.vars = "variable", variable.name = "model")
tab_fcst[,measure:="rmsfe"]
tab_init <- rbind(tab_rmse, tab_fcst)
tab_init[,lag:=7][,type:="Optimal lag order (AIC)"]
dt_plot <- rbind(dt_plot, tab_init)
dt_plot[,model:=factor(model)]
levels(dt_plot$model) <- c("Deep VAR", "VAR")
dt_plot[,measure:=factor(measure)]
levels(dt_plot$measure) <- c("RMSE", "RMSFE")

p <- ggplot(
  data=dt_plot, 
  aes(x=factor(lag), y=value, fill=model, alpha=type)
) +
  scale_fill_discrete(name="Model:") +
  geom_col(position = "dodge") +
  scale_alpha_discrete(name="Exercise:", range=c(1,0.3)) +
  facet_wrap(
    measure ~ variable,
    nrow = 2,
    scales = "free_y"
  ) +
  labs(
    x="Lag",
    y="Estimate"
  )
     
ggsave("../www/grid_benchmark.png", plot=p, width = 10, height = 4)
```

While up until now with respect to model selection we have intentionally remained strictly within the conventional VAR framework, we will now relax that constraint and vary the lag length as well as hyperparameters of the Deep VAR. In particular, we perform a grid search where we vary the number of hidden layers ($1$,$2$,$5$), number of hidden units per layer ($50$,$100$,$150$), the dropout rate ($0.3$,$0.5$,$0.7$) and the lag order ($10$, $50$, $100$). For each combination of parameter choices we train the two models and compute the various performance measures introduced above.^[Of course, with respect to the conventional VAR only the lag order affects outcomes.] Our expectation is that the conventional VAR is prone to overfitting and will produce poor out-of-sample outcomes for higher lag orders. For the Deep VAR we expect to interesting variation in the outcomes for different lag order and hyperparameter choices. It is not clear ex-ante that the Deep VAR should suffer from the same issue of overfitting for higher lag orders. The bulk of the corresponding visualizations can be found in the appendix.

### Tuning the Deep VAR

To begin with, we shall forget about benchmarking for a moment and focus on the outcomes for the Deep VAR as we vary parameters. Recall that a higher number of hidden layers (depth), a higher number of hidden units (width) and a smaller choice for the dropout rate all correspond to an increase in neural network complexity. Consistent with this intuition we find that the in-sample loss for the Deep VAR improve as complexity increases (Figure \@ref(fig:grid-rmse-train)): higher complexity leads to a reduction in bias and as we noted earlier the underlying recurrent neural networks should in principle be able to model arbitrary functions [@goodfellow2016deep]. Conversely, we observe exactly the opposite pattern for out-of-sample loss: as evident from Figure \@ref(fig:grid-rmse-test) a higher choice for the dropout rate and lower choices for the depth and width of the neural networks generally yields a smaller out-of-sample RMSE across variables. 

Interestingly, both in- and out-of-sample loss tend to decrease significantly as the number of lags increases. In other words, the Deep VAR seems to be relatively insensitive to overfitting with respect to the lag order. With that in mind, we find that using standard lag order selection tools such as the AIC above may in fact not be appropriate for Deep VARs.

Finally, Figure \@ref(fig:grid-rmsfe-test) provides an overview of how pseudo out-of-sample forecasting errors behave as we vary the hyperparameters. As before we produce one-year ahead forecasts starting from the end of the 80% training sample. In this context, the pattern is less clear and varies across variables. As the lag order increases, for example, the forecast performance for the unemployment rate deteriorates. For inflation, forecasts are poor for the medium lag choice of $p=50$ and much better for the low and high lag orders. The exact opposite relationship appears to hold for the Fed Funds Rate. With respect to the choices for the Deep VAR hyperparameters it is difficult to establish any clear pattern at all. The magnitude of differences in RMSFE is generally very small, so overall we conclude that to some extent the variation we do observe may be random. 

In light of this evidence, we propose that for the purpose of hyperparameter tuning Deep VAR researchers should focus on the RMSE associated with the 1-step ahead fitted values. For the underlying data, a reasonable set of hyperparameter choices could be: 1 hidden layer, 50 hidden units and a dropout rate of 0.5.

### Benchmark

Using the hyperparameter choices proposed above we now turn back to comparing the performance of the Deep VAR to the conventional VAR. Figure \@ref(fig:grid-benchmark) shows the pseudo out-of-sample RMSE and RMSFE for both models across the different lag choices. For the sake of completeness we also include the performance measures we obtained when we initially ran both models in section \@ref(oosample) using the optimal lag order as determined by the AIC.

The first observation is that the Deep VAR outperforms the VAR across the board, reflecting our earlier findings. As expected, the VAR is subject to overfitting for when high lag order are chosen. This trend is observed both for the RMSE as well as the RMSFE. The fact that $n$-step ahead forecasts of the VAR are also subject to overfitting with respect to the lag order, while the Deep VAR appears unaffected, to some extent may reflect what we observed earlier: for the given data, Deep VAR forecasts swiftly converge to steady levels, while VAR forecasts are volatile, which may explain the relative outperformance of the Deep VAR. It appears that this effect is amplified for higher lag orders.

```{r grid-benchmark, eval=TRUE, fig.cap="Pseudo out-of-sample RMSE and RMSFE for both models across the different lag choices. For the sake of completeness we also include the performance measures we obtained when we initially ran both models using the optimal lag order as determined by the AIC."}
knitr::include_graphics("../www/grid_benchmark.png")
```

To conclude this empirical section we summarize our main findings:

1. We provide evidence that the conventional, linear VAR fails to capture important non-linear dependencies across time and variables that are typically used to model the monetary transmission mechanism. 
2. Tapping into the broader class of Deep VAR leads to consistently better model performance. 
3. Deep VAR appears to be relatively insensitive to very high lag orders at which conventional VAR models are prone to overfitting.

```{r, eval=TRUE, include=FALSE}
knitr::opts_chunk$set(
  eval=TRUE,
  echo = FALSE,
  warning=FALSE,
  message=FALSE
)
```

\FloatBarrier