---
title: "Deep learning for multi-variate time series"
subtitle: "Proposal"
author:
- "Marc Agust√≠ (marc.agusti@barcelonagse.eu)"
- "Patrick Altmeyer (patrick.altmeyer@barcelonagse.eu)"
- "Ignacio Vidal-Quadras Costa (ignacio.vidalquadrascosta@barcelonagse.eu)"
date: "`r format(Sys.Date(), '%B, %Y')`"
output: 
  bookdown::html_document2: default
  bookdown::pdf_document2: 
    toc: false
bibliography: "../bib.bib"
link-citations: true
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(data.table)
library(ggplot2)
lapply(
  list.files("../R", pattern = "\\.R"),
  function(file) {
    source(file = file.path("../R",file))
  }
)
```

This notebook simply replicates this [blog post](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/) on multivariate time series forecasting. 

# Data

```{r}
dt <- fread("../data/pollution.csv", drop=1)
dt[,date:=as.POSIXct(sprintf("%i-%i-%i %i:00:00", year, month, day, hour))]
dt <- dt[,.(date, pm2.5, DEWP, TEMP, PRES, cbwd, Iws, Is, Ir)]
setnames(
  dt, 
  colnames(dt)[-1], 
  c('pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain')
)
for (i in names(dt))
    dt[is.na(get(i)), (i):=0]
dt <- dt[25:.N,]
dt[,wnd_dir:=NULL]
model_vars = colnames(dt)[-1]
dt[,(model_vars):=lapply(.SD, as.numeric), .SDcols=model_vars]
dt_l <- melt(dt, id.vars = "date")
```

```{r, fig.height=5, fig.width=5}
ggplot(dt_l, aes(x=date, y=value)) +
  geom_line() +
  facet_wrap(
    .~variable, 
    scales = "free_y", 
    nrow = dt_l[,length(unique(variable))]
  )
```

# LSTM (univariate output)

Instead of using only one of the time series as output variable, we simply use all of them at the same time.

```{r}
n_train_hours <- 365 * 24
lags <- 1
# Prepare X and y:
var_data <- prepare_data(dt[,.SD,.SDcols=model_vars], lags = lags, standardize = TRUE)
var_data_lstm <- prepare_lstm(var_data)
# Train-test split:
list2env(train_test_split(var_data_lstm, n_train = n_train_hours), envir = environment())
print(dim(X_train))
print(dim(y_train))
print(dim(X_test))
print(dim(y_test))
```

### Building and fitting the model

```{r}
n_units <- 50
#K <- 1
K <- length(model_vars)
dim_input <- dim(X_train)[2:3]
```

```{r}
model <- keras_model_sequential() %>% 
  layer_lstm(units = n_units, input_shape = dim_input) %>% 
  layer_dense(units = 1)
summary(model)
```


```{r}
model %>% 
  compile(
    loss = "mae",
    optimizer = "adam"
  )
```

```{r}
epochs <- 10
batch_size <- 72
history <- model %>% 
  fit(
    x = X_train, y = y_train,
    epochs = epochs,
    batch_size = batch_size,
    validation_data = list(X_test, y_test),
    verbose = 2
  )
```

### Predictions

Looking at the in-sample predictions for the first variable they generally look quite good.

```{r}
y_hat <- model %>%
  predict(X_train)
y_train_r <- array_reshape(y_train, dim=c(dim(y_train)[1],dim(y_train)[3]))
inv_y_hat <- invert_scaling(y_hat, var_data)
inv_y_hat[,type:="Prediction"]
inv_y_train <- invert_scaling(y_train_r, var_data)
inv_y_train[,type:="Actual"]
dt_train <- rbind(inv_y_hat, inv_y_train, fill=TRUE)
dt_train <- melt(dt_train, id.vars = "type")
dt_train[,date:=dt$date[1:(.N)+lags],by=.(type,variable)]
ggplot(data=dt_train, aes(x=date, y=value, colour=type)) +
  geom_line() +
  scale_color_discrete(name="Type:") +
  facet_wrap(
    ~variable, 
    scales="free_y", 
    nrow = dt_l[,length(unique(variable))]
  )
```

And similarly also for the test sample:

```{r}
y_hat <- model %>%
  predict(X_test)
y_test_r <- array_reshape(y_test, dim=c(dim(y_test)[1],dim(y_test)[3]))
inv_y_hat <- invert_scaling(y_hat, var_data, k=K)
inv_y_hat[,type:="Prediction"]
inv_y_test <- invert_scaling(y_test_r, var_data, k=K)
inv_y_test[,type:="Actual"]
dt_test <- rbind(inv_y_hat, inv_y_test, fill=TRUE)
dt_test <- melt(dt_test, id.vars = "type")
dt_test[,date:=dt$date[1:(.N)+lags],by=.(type,variable)]
ggplot(data=dt_test, aes(x=date, y=value, colour=type)) +
  geom_line() +
  scale_color_discrete(name="Type:") +
  facet_wrap(
    ~variable, 
    scales="free_y", 
    nrow = dt_l[,length(unique(variable))]
  )
```


### Mean squared error

```{r}
dt_train[,sample:="train"]
dt_test[,sample:="test"]
dt_mse <- rbind(dt_train, dt_test)
dt_mse <- dcast(dt_mse, date + variable + sample ~ type, value.var = "value")
rmse <- dt_mse[,.(rmse=sqrt(mean((Prediction-Actual)^2))), by=.(variable,sample)]
rmse
```

# MLSTM (multivariate)

```{r}
y_hat <- mlstm_$model_list[[1]] %>%
  predict(X_train)
```

