{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"understanding_navar.ipynb","provenance":[{"file_id":"1J-_OyelDr3tl2WxaVnp_FPCnRPlWsy4Z","timestamp":1619518358285}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"yr7uf9qKYO_W"},"source":["# Understanding the Python code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tVUjxE6YTnO","executionInfo":{"status":"ok","timestamp":1619515989506,"user_tz":-120,"elapsed":483,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"436226e0-356a-40a9-ebaa-4e8da41abcb5"},"source":["# Mount google drive \n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/navar "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/navar'\n","/content/drive/MyDrive/navar\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WVnfkSnOaQz5"},"source":["## Setup\n","\n","Deep learning is implemented through PyTorch. Running in this in Colab seems to work by default."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmxZXC8gZ3fK","executionInfo":{"status":"ok","timestamp":1619516004819,"user_tz":-120,"elapsed":500,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"db1f29fa-a79e-48e3-9ac5-c812d438704a"},"source":["import torch\n","x = torch.rand(5, 3)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.9787, 0.0285, 0.2613],\n","        [0.6004, 0.1945, 0.4366],\n","        [0.5072, 0.7711, 0.4547],\n","        [0.4004, 0.1937, 0.0135],\n","        [0.1877, 0.6193, 0.3672]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Bi7whh-alcW"},"source":["## NAVAR code\n","\n","Let's dive straight into the code. It seems that most Python scripts lead back to the main script [NAVAR.py](NAVAR/NAVAR.py) in one way or another.\n","\n","That file builds two different classes, `NAVAR` and `NAVARLSTM`. We will look at them one by one now."]},{"cell_type":"markdown","metadata":{"id":"PU4BiyX4bymH"},"source":["### `NAVAR`\n","\n","The `NAVAR` class takes the following inputs:\n","\n"]},{"cell_type":"code","metadata":{"id":"Zp2TsyVZaqcH"},"source":["import torch.nn as nn\n","\n","class NAVAR(nn.Module):\n","    def __init__(self, num_nodes, num_hidden, maxlags, hidden_layers=1, dropout=0):\n","        \"\"\"\n","        Neural Additive Vector AutoRegression (NAVAR) model\n","        Args:\n","            num_nodes: int\n","                The number of time series (N)\n","            num_hidden: int\n","                Number of hidden units per layer\n","            maxlags: int\n","                Maximum number of time lags considered (K)\n","            hidden_layers: int\n","                Number of hidden layers\n","            dropout:\n","                Dropout probability of units in hidden layers\n","        \"\"\"\n","        super(NAVAR, self).__init__() # inheret methods of Module class\n","\n","        self.num_nodes = num_nodes # these are the output nodes \n","        self.num_hidden = num_hidden # units per layer\n","        # MODEL ARCHITECTURE ----\n","        # This is essentially an MLP composed of multiple hidden layers, each of\n","        # them a 1D convolution.\n","        # 1) First layer\n","        self.first_hidden_layer = nn.Conv1d(num_nodes, num_hidden * num_nodes, kernel_size=maxlags,\n","                                                  groups=num_nodes)\n","        # Input tensor: \n","        self.dropout = nn.Dropout(p=dropout) # to prevent overfitting\n","        self.hidden_layer_list = nn.ModuleList() # intialize a list\n","        self.dropout_list = nn.ModuleList() # intialize a list\n","        # 2) Append remaining layers\n","        # Q: Why is kernel_size (filter?) now set to number of units? Unclear\n","        # Build the list of models:\n","        for k in range(hidden_layers - 1):\n","            self.hidden_layer_list.append(\n","                nn.Conv1d(num_nodes, num_hidden * num_nodes, kernel_size=num_hidden, groups=num_nodes))\n","            self.dropout_list.append(nn.Dropout(p=dropout))\n","        # OUTPUT ----\n","        # Penultimate layer:\n","        self.contributions = nn.Conv1d(num_nodes, num_nodes * num_nodes, kernel_size=num_hidden, groups=num_nodes)\n","        self.biases = nn.Parameter(torch.ones(1, num_nodes) * 0.0001) # hard-coded 0.0001?\n","\n","    def forward(self, x):\n","        # Initialize first hidden layer ...\n","        hidden = self.first_hidden_layer(x).clamp(min=0).view([-1, self.num_nodes, self.num_hidden])\n","        # ... and dropout:\n","        hidden = self.dropout(hidden)\n","        # FEED FORWARD ----\n","        # Then loop through following layers ....\n","        for i in range(len(self.hidden_layer_list)):\n","            hidden = self.hidden_layer_list[i](hidden).clamp(min=0).view([-1, self.num_nodes, self.num_hidden])\n","            hidden = self.dropout_list[i](hidden)\n","        # ... until reaching the penultimate layer.\n","        contributions = self.contributions(hidden)\n","        contributions = contributions.view([-1, self.num_nodes, self.num_nodes, 1])\n","        # ADDITIVE ---- \n","        predictions = torch.sum(contributions, dim=1).squeeze() #+ self.biases \n","        contributions = contributions.view([-1, self.num_nodes*self.num_nodes, 1]).squeeze()\n","        return predictions, contributions"],"execution_count":null,"outputs":[]}]}