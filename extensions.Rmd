# Caveats and extensions {#exten}

So far we have shown that Deep VARs dominate conventional VARs with respect to modelling performance. While we believe that our proposed methodology extends the conventional VAR framework quite naturally, the incorporation of non-linearities through the introduction of deep learning inevitably leads to a more complicated modelling framework. Consequently, one drawback of introducing all of the complexities of deep learning into the VAR framework is that previously simply tasks related to inference and uncertainty quantification are now more difficult. For example, confidence intervals around point forecasts from a linear VAR can be computed using a closed-form analytical solution [@kilian2017structural]. No such formulas exist in the context of our proposed Deep VARs.

But as we already mentioned above, policy-makers rarely base their decisions solely on point estimates, so ultimately our proposed Deep VAR framework needs to be able to quantify uncertainty around estimates. Future work on this project is likely to focus on this in the first place. Quantifying uncertainty in the Deep VAR contest will involve non-parametric bootstrapping or other forms of Monte Carlo methods. To readers familiar with the conventional VAR framework, this should to some extent sound familiar since confidence intervals around impulse response functions are typically bootstrapped. For the Deep VAR, one way to quantify uncertainty around estimates is to rely on Bayesian neural networks and use dropout not only during training but also at the inference stage. This is commonly referred to as **Monte Carlo dropout** and has been used by @kendall2017uncertainties and others to this end. Monte Carlo dropout is typically interpreted as a kind of approximate inference algorithm for Bayesian neural networks. Intuitively, it may help to think of Monte Carlo dropout as producing a distribution over point estimates through the introduction of stochasticity.

Support for the estimation of impulse response functions is another missing cornerstone in the current version of our proposed framework to which future research should be dedicated. IRFs are used to infer how system variables changes in response to unit shocks to any one of the system variables. When estimating the model with the traditional VAR, IRFs can be derived from the reduced form model coefficients. Provided that all the roots of the autoregressive polynomial lie outside the unit circle, this derivation boils down to transforming the model into its moving average representation and plugging the shock into the vector moving average (VMA) representation. Generating IRFs is more difficult in the Deep VAR setting given that we cannot recover the Deep VMA representation. But while IRFs cannot be retrieved analytically,  @verstyuk2020modeling manages to recover them numerically and his proposed methodology should be readily applicable to our Deep VAR framework.

Within the VAR framework, it is also common to investigate Forecast Error Variance Decompositions (FEVD). This allows researchers or policy-makers to quantify the percentage of the variability of the forecast error of one variable that is explained by other variables in the system. Again, this can be done analytically in the context of linear VARs. It is less obvious how to recover such decompositions from the Deep VAR, but our intuition is that it will once again involve numerical methods, in particular Monte Carlo dropout as already discussed above.

Apart from having a framework that can be used to do inference, policy-makers are typically also concerned about the general interpretability and transparency of the models they use. The linear relationship between inputs and outputs imposed by the conventional VAR can be easily digested not only practitioners, but potentially also their non-expert audiences. In the case of the Deep VAR model, the underlying deep neural networks are **black boxes**: they are inherently non-transparent. At the time of writing this black-box issue is still largely unresolved, but research into explainable AI (**XAI**) has been growing rapidly [@arrieta2020explainable, @fan2020interpretability]. Much like in the context of uncertainty quantification, the Bayesian approach to neural networks appears to be most promising when it comes to interpretable deep learning. For example, @ish2019interpreting propose a simple entropy-based measure for feature importance in Bayesian neural networks, which helps to facilitate post-hoc model interpretability.

While we very much recognize the need for model interpretability especially in the context of policy-making, we believe that the Deep VAR framework proposed here can be augmented to meet these demands. In light of the Deep VAR's dominance over conventional VARs when it comes to modelling performance, we believe that it is certainly worth pursuing further research in this realm. 




