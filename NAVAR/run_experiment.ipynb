{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_experiment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsB4meIbOwC9hplLy2mGp/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lfufqPSyYsOy"},"source":["# Understanding the NAVAR code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJg16zRufI-j","executionInfo":{"status":"ok","timestamp":1620138159852,"user_tz":-120,"elapsed":28840,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"3da29463-5f7a-4b51-fbf2-d34d11f84729"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/navar/NAVAR"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/navar/NAVAR\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AesSQ0etZBVh"},"source":["Installing Bussman's requirements:"]},{"cell_type":"code","metadata":{"id":"56wCXw8vfVkP"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hRufPLJfqFS","executionInfo":{"status":"ok","timestamp":1620138186073,"user_tz":-120,"elapsed":7710,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["from train_NAVAR import train_NAVAR\n","from evaluate import calculate_AUROC, dream_file_to_causal_matrix\n","import pandas as pd\n","import argparse"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IUD36OvxZUjf"},"source":["To understand the code better, we will just run a single experiment, in particular the one called `ecoli1`."]},{"cell_type":"code","metadata":{"id":"bm4q0aJDfs6A","executionInfo":{"status":"ok","timestamp":1620138188311,"user_tz":-120,"elapsed":588,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["experiment = 'ecoli1' # experiment name\n","# should accuracy be evaluated?\n","evaluate = True\n","# should we use LSTM?\n","lstm = True \n","lambda1 = 0.26025947107502856\n","batch_size = 46 # batch size for training\n","wd = 1.4961159190152877e-05\n","hidden_nodes = 10 # random (think width of layer)\n","learning_rate = 0.002 \n","hl = 1 # number of hidden layers (LSTM: number of recurrent states)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9Z72hwMfwXw","executionInfo":{"status":"ok","timestamp":1620138193413,"user_tz":-120,"elapsed":2091,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# load the data\n","file = f'experiments/{experiment}.tsv'\n","ground_truth_file = f'experiments/{experiment}_gt.txt'\n","data = pd.read_csv(file, sep='\\t')\n","data = data.values[:, 1:]\n","epochs = 100\n","maxlags = 21 if lstm else 2"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yXGo_fIZ3vy"},"source":["Let's explore that data for a moment. The `ecoli1` data is made up by 966 rows and 100 columns."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHqf5XfBZqAc","executionInfo":{"status":"ok","timestamp":1620138196967,"user_tz":-120,"elapsed":1237,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"64dfb6a8-d51d-4970-d328-b4274a5a0ca3"},"source":["print(data.shape)\n","data"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(966, 100)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.15213198, 0.05465604, 0.04711906, ..., 0.13579787, 0.39367317,\n","        0.39847743],\n","       [0.2012258 , 0.08574432, 0.20401708, ..., 0.23993862, 0.48825811,\n","        0.35958879],\n","       [0.19322354, 0.02080837, 0.32134769, ..., 0.27688198, 0.5669294 ,\n","        0.37422256],\n","       ...,\n","       [0.13009743, 0.09215208, 0.45135629, ..., 0.42214107, 0.93147055,\n","        0.48743001],\n","       [0.21391873, 0.01804852, 0.48076942, ..., 0.40894357, 0.85753113,\n","        0.43878193],\n","       [0.17129875, 0.0600136 , 0.43334571, ..., 0.4263017 , 0.85685361,\n","        0.50165747]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFuWfxDYf4Se","executionInfo":{"status":"ok","timestamp":1620132429103,"user_tz":-120,"elapsed":84569,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"6e4aa06a-aa2c-4f05-8a10-c4bf68168f74"},"source":["%%time\n","# start training\n","print(f\"Starting training on the data from experiment {experiment}, training for {epochs} iterations.\")\n","score_matrix, _, _ = train_NAVAR(data, maxlags=maxlags, hidden_nodes=hidden_nodes, dropout=0, epochs=epochs,\n","                                 learning_rate=learning_rate, batch_size=batch_size, lambda1=lambda1,\n","                                 val_proportion=0.0, weight_decay=wd, check_every=500, hidden_layers=hl, normalize=True,\n","                                 split_timeseries=21, lstm=lstm)\n","# evaluate\n","print('Done training!')\n","if evaluate:\n","    ground_truth_matrix = dream_file_to_causal_matrix(ground_truth_file)\n","    AUROC = calculate_AUROC(score_matrix, ground_truth_matrix, ignore_self_links=True)\n","    print(f\"The AUROC of this model on experiment {experiment} is: {AUROC}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Starting training on the data from experiment ecoli1, training for 100 iterations.\n","Done training!\n","The AUROC of this model on experiment ecoli1 is: 0.5700026598465473\n","CPU times: user 52.6 s, sys: 23.8 s, total: 1min 16s\n","Wall time: 1min 23s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SvCEO415bhS8","executionInfo":{"status":"ok","timestamp":1620138202964,"user_tz":-120,"elapsed":1360,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# Assgin parameters:\n","dropout=0\n","check_every=500\n","split_timeseries=21\n","val_proportion=0.1\n","normalize=True\n","hidden_layers=hl\n","weight_decay=wd"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTHhsEVBb-H4","executionInfo":{"status":"ok","timestamp":1620138206804,"user_tz":-120,"elapsed":720,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["import torch\n","from NAVAR import NAVARLSTM\n","# T is the number of time steps, N the number of variables\n","T, N = data.shape\n","# instantiate the NAVAR model\n","model = NAVARLSTM(N, hidden_nodes, maxlags, dropout=dropout, hidden_layers=hidden_layers)\n","# use Mean Squared Error and the Adam optimzer\n","criterion = torch.nn.MSELoss(reduction='mean')\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGsDI_6ldXjn"},"source":["## Data loading"]},{"cell_type":"markdown","metadata":{"id":"Ndo0xBC3j9ph"},"source":["### Prepare data"]},{"cell_type":"code","metadata":{"id":"t-sLSQrLdW-f","executionInfo":{"status":"ok","timestamp":1620138212109,"user_tz":-120,"elapsed":920,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["data_raw = data\n","data = torch.from_numpy(data)\n","\n","# normalize every variable to have 0 mean and standard deviation 1 -- necessary?\n","if normalize:\n","    data = data / torch.std(data, dim=0)\n","    data = data - data.mean(dim=0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F18U5ldeeKC0","executionInfo":{"status":"ok","timestamp":1620138215349,"user_tz":-120,"elapsed":962,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"67549bf9-5f93-4f5e-939f-69cdd800df56"},"source":["# initialize our input and target variables\n","X = torch.zeros((int(T/maxlags), maxlags, N)) # input \n","X.shape # what does the first dim mean?"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([46, 21, 100])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"-ylYZeBBfKxT","executionInfo":{"status":"ok","timestamp":1620138217829,"user_tz":-120,"elapsed":459,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# X and Y consist of timeseries of length K\n","for i in range(int(T/maxlags) -1):\n","    X[i, :, :] = data[i*maxlags:(i+1)*maxlags, :] # batches?"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MURUHeD9f8pG","executionInfo":{"status":"ok","timestamp":1620138221964,"user_tz":-120,"elapsed":703,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"44c36b06-2e29-432d-8a0c-ee7962e6862a"},"source":["X = X.permute(0, 2, 1) # just transposes matrices\n","X.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([46, 100, 21])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"6yKYod4RgjER","executionInfo":{"status":"ok","timestamp":1620138236401,"user_tz":-120,"elapsed":1100,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["X.view(-1, N, maxlags)\n","Y = X[:, :, 1:]\n","X = X[:, :, :-1]"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PuaEZCFfj_zk"},"source":["### Train test split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmSOfaI0kCi2","executionInfo":{"status":"ok","timestamp":1620138239702,"user_tz":-120,"elapsed":644,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}},"outputId":"501b91b0-bb81-4c60-ce77-88c43a39adec"},"source":["import numpy as np\n","number_of_val_indices = np.int(np.floor(val_proportion * Y.shape[0]))\n","number_of_val_indices"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"3r6AJo-3k0vP","executionInfo":{"status":"ok","timestamp":1620138245428,"user_tz":-120,"elapsed":1394,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["train_indices = np.arange(Y.shape[0] - number_of_val_indices)\n","val_indices = np.arange(Y.shape[0] - number_of_val_indices, Y.shape[0])"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PNU7ke5lIPA","executionInfo":{"status":"ok","timestamp":1620138249264,"user_tz":-120,"elapsed":1172,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["if val_proportion == 0:\n","    X_train = X\n","    Y_train = Y\n","else:\n","    X_train = X[train_indices]\n","    Y_train = Y[train_indices]\n","    X_val = X[val_indices]\n","    Y_val = Y[val_indices]"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Yb0E7komx8n"},"source":["## Back to training\n","\n","Check of cuda GPU available and move to it. Available on Colab yay"]},{"cell_type":"code","metadata":{"id":"5F3HJiPYm2KL","executionInfo":{"status":"ok","timestamp":1620138268220,"user_tz":-120,"elapsed":11667,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# push model and data to GPU if available\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    X_train = X_train.cuda()\n","    Y_train = Y_train.cuda()\n","    if X_val is not None:\n","        X_val = X_val.cuda()\n","        Y_val = Y_val.cuda()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"oci2hspUnP8Y","executionInfo":{"status":"ok","timestamp":1620138271753,"user_tz":-120,"elapsed":929,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# Initialize\n","num_training_samples = X_train.shape[0]\n","total_loss = 0\n","loss_val = 0"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCEb6Aagnvol"},"source":["This is where the actual training happens now."]},{"cell_type":"code","metadata":{"id":"cF3jdYJanYG9"},"source":["# start of training loop\n","batch_counter = 0\n","for t in range(1, epochs+1): # loop over epochs / iterations\n","    #obtain batches\n","    batch_indeces_list = []\n","    # if desired number of batches for training is smaller than \n","    # the number of training samples, then enter here:\n","    if batch_size < num_training_samples: \n","        batch_perm = np.random.choice(num_training_samples, size=num_training_samples, replace=False)\n","        for i in range(int(num_training_samples/batch_size) + 1):\n","            start = i*batch_size\n","            batch_i = batch_perm[start:start+batch_size]\n","            if len(batch_i) > 0:\n","                batch_indeces_list.append(batch_perm[start:start+batch_size])\n","    else:\n","        batch_indeces_list = [np.arange(num_training_samples)]\n","\n","    # Now train for each training batch:\n","    for batch_indeces in batch_indeces_list:\n","        batch_counter += 1 # increase counter\n","        X_batch = X_train[batch_indeces] # get training data\n","        Y_batch = Y_train[batch_indeces] # ...\n","        \n","        # forward pass to calculate predictions and contributions\n","        predictions, contributions = model(X_batch) # return predictions and contributions\n","\n","        # calculate the loss\n","        loss_pred = criterion(predictions, Y_batch) # basically loss on 1-step ahead prediction\n","        loss_l1 = (lambda1/N) * torch.mean(torch.sum(torch.abs(contributions), dim=1))\n","        loss = loss_pred + loss_l1\n","        total_loss += loss\n","\n","        # Zero gradients, perform a backward pass, and update the weights.\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # every 'check_every' epochs we calculate and print the validation loss\n","    if t % check_every == 0:\n","        model.eval()\n","        if val_proportion > 0.0:\n","            val_pred, val_contributions = model(X_val)\n","            loss_val = criterion(val_pred, Y_val)\n","        model.train()\n","\n","        print(f'iteration {t}. Loss: {total_loss/batch_counter}  Val loss: {loss_val}')\n","        total_loss = 0\n","        batch_counter = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRdO63cTwYfK"},"source":["For a single epoch ..."]},{"cell_type":"code","metadata":{"id":"Qtnfd1iGwF9S","executionInfo":{"status":"ok","timestamp":1620140876051,"user_tz":-120,"elapsed":595,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# start of training loop\n","batch_counter = 0\n","t = 1\n","#obtain batches\n","batch_indeces_list = []\n","# if desired number of batches for training is smaller than \n","# the number of training samples, then enter here:\n","if batch_size < num_training_samples: \n","    batch_perm = np.random.choice(num_training_samples, size=num_training_samples, replace=False)\n","    for i in range(int(num_training_samples/batch_size) + 1):\n","        start = i*batch_size\n","        batch_i = batch_perm[start:start+batch_size]\n","        if len(batch_i) > 0:\n","            batch_indeces_list.append(batch_perm[start:start+batch_size])\n","else:\n","    batch_indeces_list = [np.arange(num_training_samples)]"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ohGVsjnYxRBy"},"source":["... for a single batch:\n","\n","> Here we noted a weird thing: the batch size here really refers to how many blocks of size ($N \\times$ `maxlags`) we want to include in each batch. So the number of total observations per batch is really the specified `batch_size` multiplied by the specified maximum number of lags `maxlags`. We noted that depending on specifications only a single or very few train-test splits will be performed in each epoch. Does that cause an inefficiency?\n","\n","For example, for his choices, the first and only batch here simply corresponds to all of the training data."]},{"cell_type":"code","metadata":{"id":"4XQ6rgg95jL2","executionInfo":{"status":"ok","timestamp":1620140927042,"user_tz":-120,"elapsed":1405,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["batch_counter += 1 # increase counter\n","X_batch = X_train[batch_indeces] # get training data\n","Y_batch = Y_train[batch_indeces] # ..."],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-h-cymEy7a5c"},"source":["Now we will go into the model:"]},{"cell_type":"code","metadata":{"id":"DpfuIwVO7TIX","executionInfo":{"status":"ok","timestamp":1620142541512,"user_tz":-120,"elapsed":1556,"user":{"displayName":"Patrick Altmeyer","photoUrl":"","userId":"16341228965716438767"}}},"source":["# forward pass to calculate predictions and contributions\n","predictions, contributions = model(X_batch) # return predictions and contributions\n","\n","# calculate the loss\n","loss_pred = criterion(predictions, Y_batch) # basically loss on 1-step ahead prediction\n","loss_l1 = (lambda1/N) * torch.mean(torch.sum(torch.abs(contributions), dim=1))\n","loss = loss_pred + loss_l1\n","total_loss += loss\n","\n","# Zero gradients, perform a backward pass, and update the weights.\n","optimizer.zero_grad()\n","loss.backward()\n","optimizer.step()"],"execution_count":46,"outputs":[]}]}