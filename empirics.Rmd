# Empirical results {#empirics}

```{r, echo = FALSE, warning=FALSE,message=FALSE}
dt <- readRDS("data_VAR/preprocessed.rds")
var_cols <- colnames(dt)[2:ncol(dt)]
```

```{r}
# Choosing lags:
max_lags <- 12
lags <- lag_order(dt, max_lag = max_lags)$p
```

```{r, echo = FALSE, warning=FALSE, message=FALSE, eval=FALSE}
# VAR fitting
var_model <- vareg(dt, lags = lags)
# Deep VAR fitting
deepvar_model <- deepvareg(dt, lags=lags, num_units = 100, epochs=100, p_drop_out = 0.5)
```

```{r, eval=FALSE}
# Cum RMSE plot
cum_loss_var <- cum_loss(var_model)$cum_loss[,type:="VAR"]
cum_loss_dvar <- cum_loss(deepvar_model)$cum_loss[,type:="Deep VAR"]
dt_plot <- rbind(cum_loss_dvar, cum_loss_var)
p <- ggplot2::ggplot(data=dt_plot, ggplot2::aes(x=date, y=value, colour=type)) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(~variable, scales = "free_y") +
  ggplot2::scale_color_discrete(name="Model:") +
  ggplot2::labs(
      x="Date",
      y="Squared error"
    ) +
  geom_hline(aes(yintercept=0), lwd=0.1)
ggsave(
  filename = "www/cum_loss_full.png", 
  plot = p,
  width = 6,
  height = 4
)
```

```{r}
resids <- rbind(
  deepvar_model$res[,model:="Deep VAR"],
  data.table(var_model$res)[,model:="VAR"]
)
resids <- melt(resids, id.vars = "model")
p <- ggplot(data=resids, aes(sample=value)) +
  stat_qq() + stat_qq_line() +
  facet_wrap(model~variable, scales="free_y", nrow=2) +
  labs(
    x="Theoretical",
    y="Sample"
  )
ggsave(
  filename = "www/qqplot_full.png", 
  plot = p,
  width = 8,
  height = 4
)
```

```{r, eval=FALSE}
lag_max <- 25
ci <- .95
resids_acf <- resids[
  ,
  .(
    acf=stats::acf(value, lag.max = lag_max, plot=FALSE)$acf[,,1], 
    lag=stats::acf(value, lag.max = lag_max, plot=FALSE)$lag[,,1],
    ci=qnorm((1 + ci)/2)/sqrt(.N)
  ),
  by=.(model, variable)
]
p <- ggplot(data=resids_acf, aes(x=lag, y=acf)) +
  geom_hline(aes(yintercept = ci), linetype = "dashed", color = "darkblue") +
  geom_hline(aes(yintercept = 0)) +
  geom_hline(aes(yintercept = -ci), linetype = "dashed", color = "darkblue") +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  facet_wrap(model~variable, scales="free_y", nrow=2) +
  labs(
    y="ACF",
    x="Lag"
  ) 
ggsave(
  filename = "www/acf_full.png", 
  plot = p,
  width = 8,
  height = 4
)
```

We now proceed to benchmark the proposed Deep VAR model against the conventional VAR using out macroeconomic time series data. To begin with, we compare both models in terms of their in-sample fit. For this part of the analysis the models will be strictly run under the same framing conditions. Due to the RNN's capacity to essentially model any possible function $f_i(\cdot)$ the Deep VAR dominates the VAR in this realm. We investigate during what time periods the outperformance of the Deep VAR is particurly striking to gain a better understanding of when and why it pays off to relax the linearity constraint.

These findings with respect to in-sample performance provide some initial evidence in favor of the Deep VAR. But since a reduction in modelling bias is typically associated with an increase in variance, we are particularly interested in benchmarking the models with respect to their out-of-sample performance. To this end we split our sample into train and test subsamples. We then firstly benchmark the models in terms of their pseudo out-of-sample fit. Finally we also look at model performance with respect to $n$-step ahead pseudo out-of-sample forecasts.

The final part of this section relaxes the constraint on the framing conditions. In particular, we investigate how hyperparameter tuning with respect to the neural network architecture and lag length $p$ can improve the performance of the Deep VAR.

## In-sample fit

To assess the fit of our models we use the root mean squared error (RMSE) as our preferred loss function. Figure \@ref(fig:cum-loss-full) shows the cumulative RMSE of both the VAR model and Deep-VAR model for each of the time series over the whole sample period. We have decided to include the post-Covid sample period despite the associated structural break, since it serves as interesting point of comparison. The first thing we can observe is that the RMSE of the Deep VAR is consistently flatter than the RMSE of the VAR. With respect to in-sample performance, the Deep VAR the VAR throughout the entire time period of the experimental analysis and for all of the considered variable. This empirical observation seems to confirm our expectation that the vector autoregressive process is characterized by important non-linear dependencies across time and variables that the conventional VAR fails to capture. 

```{r cum-loss-full, fig.cap="Comparison of cumulative loss over the entire sample period for conventional VAR and proposed Deep VAR."}
knitr::include_graphics("www/cum_loss_full.png")
```

Figure \@ref(fig:cum-loss-full) is especially useful to asses in which specific periods the Deep VAR model achieves better modelling outcomes than the VAR model. From the very beginning and across variables, we observe that the increase in cumulative loss for the VAR model is greater than for the Deep VAR model. We can ndicating that even in normal economic times, soft non linear relationships may be present in the series.

The first interesting development we can observe is at the beginning of the 1980s. During the beginning of this decade, the cumulative RMSE of the Deep VAR almost remains constant (it slope is close to zero), therefore, it almost makes no error. On the other hand, the RMSE of the VAR keeps increasing. For the case of the IP and the UR, the RMSE of the VAR  maintains its previous slope, meaning no improvement nor deterioration appears. Whereas for the CPI, it becomes approximately as twice as big, hence doubling its error, and for the FFR it is approximately quadrupled.

This can be explained by the early 1980s recession. This recession is considered to be the most sever since World War II. As Figure \@ref(fig:cum-loss-full) shows, the Deep-VAR model almost perfectly fits the data in this period. This is due to the fact that economic recessions have a very high component of non linear relationships, therefore not only the Deep-VAR does better than the VAR, but it almost accomplishes a perfect fit. This idea is then reinforced by the poor performance of the VAR in this period for the CPI and FFR series. Both series, specially the FFR, are very volatile in this period. This means that non linear relationships are present and therefore, the Deep-VAR model remains unaffected by the volatility, while the VAR model struggles a lot to fit the data due to its limitations for fitting data generating processes with non linear components.

For the case of the 2007 recession we can see a pattern similar to the one for the recession of the early 1980: the Deep-VAR achieves a better fit because it can correctly identify the pattern soon enough. Furthermore, in the recovery time, the Deep-VAR cumulative RMSE still has a lower slope than the VAR cumulative RMSE. 

Finally, we can compare how both models perform in a period with highly non linear components, the Covid era. It can be clearly seen that, for both IP and UR, that is, the series that really changed its behavior with Covid, the VAR model struggles a lot to fit the data. For the VAR model, both series add in a few months approximately 50% of the error tehy had been accumulating for the previous 60 years. Whereas in the case of the Deep-VAR model, they only accumulate approximately 30% of the error they had been accumulating before. This, once again, reinforces our hypothesis that the Deep-VAR model structure helps dealing with non linarities on the data generating process and therefore fitting the data better on periods where non-linareities are present, which means that the Deep-VAR model can be extremely useful for recession periods.

As a santiy check we also visually inspected the distributional properties of the model residuals for the full-sample fit. The outcomes are broadly consistent across models: while for some variables residuals are clearly not Gaussian, we see no evidence of serial autocorrelation of residuals. Visualizations can be found in section \@ref(#resids) of the appendix.

\FloatBarrier

## Out-of-sample fit

```{r, echo = FALSE, warning=FALSE,message=FALSE}
#Train test split
train_test_split <- split_sample(dt)
train_data <- train_test_split$train_data
```


```{r, echo = FALSE, warning=FALSE,message=FALSE}
#Select the number of lags
lags <- lag_order(train_data)$p
```


```{r, echo = FALSE, warning=FALSE,message=FALSE, eval=TRUE}
# VAR
var_model <- vareg(train_data, lags = lags)
saveRDS(var_model, file="results/var_model_train.rds")
# Deep VAR
deepvar_model <- deepvareg(train_data, lags=lags, num_units = 100, epochs=100, p_drop_out = 0.5)
saveRDS(deepvar_model, file="results/deepvar_model_train.rds")
```


```{r, eval=FALSE}
var_model <- readRDS("results/var_model_train.rds")
deepvar_model <- readRDS("results/deepvar_model_train.rds")
```

```{r, echo = FALSE, warning=FALSE,message=FALSE}
y_true <- var_model$y_train
```

Now, we split the data into training data and test data. The model will be fitted in the training dataset and then we assess the performance of the model in the test dataset. This is done in order to see the performace of both models on data which is independent of the data that has been used to fit the model. The training dataset goes from March 1959 to November 2007, whereas the test data goes from December 2007 to March 2021, so we train the model on 80 percent of the data and we test the model on the remaining 20 percent. 

```{r, echo = FALSE, warning=FALSE,message=FALSE, results=FALSE, include = FALSE}
#VAR
pred_var <- predict(var_model)
plot(pred_var, y_true = y_true)
```

```{r, echo = FALSE, warning=FALSE,message=FALSE, results=FALSE, include = FALSE}
#DeepVAR
pred_deepvar <- predict(deepvar_model)
plot(pred_deepvar, y_true = y_true)
```


```{r, echo = FALSE, warning=FALSE,message=FALSE,results=FALSE, include = FALSE}
#Predictions out of sample
X_test <- prepare_test_data(train_test_split, lags=lags)$X_test
y_test <- prepare_test_data(train_test_split, lags=lags)$y_test
```

```{r, echo = FALSE, warning=FALSE,message=FALSE,results=FALSE, include = FALSE}
#VAR
pred_var <- predict(var_model, X=X_test)
plot(pred_var, y_true = y_test)
```

```{r, echo = FALSE, warning=FALSE,message=FALSE,results=FALSE, include = FALSE}
#DeepVAR
pred_dvar <- predict(deepvar_model, X=X_test)
plot(pred_dvar, y_true = y_test)
```

```{r, echo = FALSE, results = FALSE, warning=FALSE,message=FALSE, include = FALSE}
#RMSE 
rmse_var <- rbind(
  rmse(var_model)[,model:="var"][,sample:="train"],
  rmse(var_model, X=X_test, y=y_test)[,model:="var"][,sample:="test"]
)
rmse_dvar <- rbind(
  rmse(deepvar_model)[,model:="deepvar"][,sample:="train"],
  rmse(deepvar_model, X=X_test, y=y_test)[,model:="deepvar"][,sample:="test"]
)
tab_rmse <- rbind(rmse_var, rmse_dvar)
tab_rmse <- data.table::dcast(tab_rmse, sample + variable ~ model, value.var = "value")
tab_rmse[, ratio:= deepvar/var]
knitr::kable(tab_rmse, 
  col.names = c("Sample", "Variable", "DVAR", "VAR", "Ratio"),
  digits = 5) 
```

The following table shows ths Root Mean Squared Error (RMSE) for the in-sample and the out-of-sample predictions of both the VAR model and the Deep-VAR model. We can see that the RMSE for the Deep VAR outperforms the one for the classic VAR for both the training data and the test data and for all time series. The fifth column of the table shows us the ratio between the Deep-VAR and the VAR. The lower the ratio, the better the Deep-VAR with respect to the VAR. Note that for the industrial production and for the unemployment rate the Deep-VAR does fairly well compared to the VAR.

```{r, echo = FALSE, results='asis', warning=FALSE,message=FALSE}
library(xtable)
dtf<- xtable(tab_rmse[, -"Mean"], caption = c("RMS"))
names(dtf) <- c('Sample','Variable', 'DeepVAR', "VAR", "Ratio")
bold <- function(x){
paste('{\\textbf{',x,'}}', sep ='')
}
italic <- function(x){
paste0('{\\emph{ ', x, '}}')
}
print(dtf, sanitize.colnames.function = bold, sanitize.rownames.function = italic, booktabs = TRUE, include.rownames=FALSE)
```


```{r, echo = FALSE, warning=FALSE,message=FALSE}
#Cumulative loss in-sample

cum_loss_var <- cum_loss(var_model)$cum_loss[,type:="var"]
cum_loss_dvar <- cum_loss(deepvar_model)$cum_loss[,type:="deepvar"]
dt_plot <- rbind(cum_loss_dvar, cum_loss_var)
ggplot2::ggplot(data=dt_plot, ggplot2::aes(x=date, y=value, colour=type)) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(~variable, scales = "free_y") +
  ggplot2::scale_color_discrete(name="Model:") +
  ggplot2::labs(
      x="Date",
      y="Squared error"
    )
```



```{r, echo = FALSE, warning=FALSE,message=FALSE}
#Cumulative loss out-of-sample
cum_loss_var <- cum_loss(var_model)$cum_loss[,type:="var"]
cum_loss_dvar <- cum_loss(deepvar_model)$cum_loss[,type:="deepvar"]
dt_plot <- rbind(cum_loss_dvar, cum_loss_var)
ggplot2::ggplot(data=dt_plot, ggplot2::aes(x=date, y=value, colour=type)) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(~variable, scales = "free_y") +
  ggplot2::scale_color_discrete(name="Model:") +
  ggplot2::labs(
      x="Date",
      y="Squared error"
    )
```

## Pseudo out-of-sample forecasts

```{r}
n_ahead <- 12
y_true <- y_test[1:n_ahead,]
```

In the following, we will compare the forecast performance of the VAR and the Deep-VAR. To do that, we compute the 1-step ahead forecast for one year. The dashed line represents the forecast and the solid black line represents the actual value of the time series. To compare the performance of both methods, we compute the Forecast RMSE.

```{r, echo = FALSE, warning=FALSE,message=FALSE}
#Forecasts
#VAR
fcst_var <- forecast(var_model, n.ahead = n_ahead)
plot(fcst_var, y_true=y_true, history = 20)
```


```{r, echo = FALSE, warning=FALSE,message=FALSE}
#DeepVAR
fcst_dvar <- forecast(deepvar_model, n.ahead = n_ahead)
plot(fcst_dvar, y_true=y_true, history = 20)
```

In the following table, we look more specifically at this comparison. In particular, we compute the Forecast RMSE for the VAR method and for the Deep-VAR, and we also compute the correlation of the forecast with the actual value of each time series.

```{r, echo = FALSE, warning=FALSE,message=FALSE, include= FALSE}
rmsfe(fcst_dvar, y_true = y_true)
rmsfe(fcst_var, y_true = y_true)
correlations <- function(fcst_var,fcst_dvar,y_true){
  list_names <- c(colnames(y_true))
  cor_var <- c()
  cor_dvar <- c()
  for (n in list_names){
    cor_var <- c(cor_var,cor(fcst_var$fcst[, ..n][-1], y_true[,n]))
    cor_dvar <- c(cor_dvar,cor(fcst_dvar$fcst[, ..n][-1], y_true[,n]))
    
  }
  return(data.table(cor_var,cor_dvar))
}
correlations <- correlations(fcst_var,fcst_dvar,y_true)
```

```{r, results = 'asis', echo = FALSE, warning=FALSE,message=FALSE}
tab_fcst <- data.table("Variable" = colnames(y_true), "VAR FRMSE"= rmsfe(fcst_var, y_true = y_true)[,value] , "Deep-VAR FRMSE"= rmsfe(fcst_dvar, y_true = y_true)[,value],
                       "VAR correlations" =correlations$cor_var ,
                       "Deep-VAR correlations" = correlations$cor_dvar)
dtf<- xtable(tab_fcst, caption = c("Forecasts results"))
print(dtf, sanitize.colnames.function = bold, sanitize.rownames.function = italic, booktabs = TRUE, include.rownames=FALSE,size = "\\setlength{\\tabcolsep}{2pt}")
```

As we can see in the table, the Forecast RMSE of the Deep-VAR is lower than the one for the VAR for unemployment rate and for the federal funds rate, and it is the same for industrial production and inflation. Hence, the Deep-VAR outperforms the VAR in terms of forecasts. It is also of interest to analyze the correlation between the forecasts and the actual values. In the case of the VAR, there is, for all time series, a negative correlation, that is, when the time series evolves in one direction, the VAR forecast evolves in the opposite direction. However, this is not true for the Deep-VAR. For industrial production, is certainly true that the Deep-VAR forecast has a highly negative correlation with the actual values, but achives the same forecasting performance than the one for the VAR. However, for the rest of time series, the Deep-VAR has a positive correlation, that is, the forecast generally evolves in the same direction as the actuial values. Note that this positive correlation cannot be too strong given that the forecasts at some point in time return to the mean, and hence, have no varaiability.  

## Hyper parameter tuning

```{r, include= FALSE, eval=FALSE, echo = FALSE}
# Parameters
n_ahead <- 12
layers <- c(1,2,5)
units <- c(50,100,150)
dropout <- c(0.3,0.5,0.7)
lags <- c(10,50,100)
grid <- data.table::CJ(
  layer = layers,
  unit = units,
  dropout = dropout
)
n_sim <- length(lags) * grid[,.N]
```

```{r, include= FALSE, eval=FALSE, echo = FALSE}
# Train test:
train_test_split <- split_sample(dt)
train_data <- train_test_split$train_data
```

<<<<<<< HEAD
```{r, eval=FALSE}
=======
```{r,  include= FALSE, eval=FALSE, echo = FALSE}
>>>>>>> 56a14baddbded9a083e1facc411bae7a34afc8e7
grid_search <- rbindlist(
  lapply(
    1:length(lags),
    function(lag_idx) {
      
      p <- lags[lag_idx]
      
      message(sprintf("Running for lag=%i", p))
      
      # For given lag fit VAR:
      var_model <- vareg(train_data, lags = p) 
      y_true <- var_model$y_train
      
      # Test data:
      X_test <- prepare_test_data(train_test_split, lags=p)$X_test
      y_test <- prepare_test_data(train_test_split, lags=p)$y_test
      y_true_fcst <- y_test[1:n_ahead,]
      
      # Enter loop for Deep VAR:
      results <- rbindlist(
        lapply(
          1:nrow(grid),
          function(i) {
            
            list2env(c(grid[i,]), envir = environment()) # retrieve parameters
            
            pct_done <- ( ( (lag_idx-1)*grid[,.N] + i ) / n_sim ) * 100
            message(sprintf("Percent done: %0.2f", pct_done))
            
            # Fit Deep VAR model
            deepvar_model <- deepvareg(
              train_data, 
              lags=p, 
              num_units = unit,
              num_layers = layer,
              epochs=100,    
              p_drop_out = dropout
            )
            
            # Train RMSE
            train_rmse <- rbind(
              rmse(deepvar_model)[,model:="dvar"][,measure:="rmse"][,sample:="train"],
              rmse(var_model)[,model:="var"][,measure:="rmse"][,sample:="train"]
            )
            
            # Test RMSE
            test_rmse <- rbind(
              rmse(
                deepvar_model, X=X_test, y=y_test
              )[,model:="dvar"][,measure:="rmse"][,sample:="test"],
              rmse(
                var_model, X=X_test, y=y_test
              )[,model:="var"][,measure:="rmse"][,sample:="test"]
            )
            
            # Forecasts RMSE
            fcst_dvar <- forecast(deepvar_model, n.ahead = n_ahead)
            fcst_var <- forecast(var_model, n.ahead = n_ahead)
            fcst_rms <- rbind(
              rmsfe(
                fcst_dvar, y_true = y_true_fcst
              )[,model:="dvar"][,measure:="rmsfe"][,sample:="test"],
              rmsfe(
                fcst_var, y_true = y_true_fcst
              )[,model:="var"][,measure:="rmsfe"][,sample:="test"]
            )
            
            # Forecasts correlations:
            fcst_corr <- rbind(
              cor_fcst(
                fcst_dvar, y_true = y_true_fcst
              )[,model:="dvar"][,measure:="corr"][,sample:="test"],
              cor_fcst(
                fcst_var, y_true = y_true_fcst
              )[,model:="var"][,measure:="corr"][,sample:="test"]
            )
            
            # Put together:
            results <- rbind(
              train_rmse, 
              test_rmse, 
              fcst_rms, 
              fcst_corr
            )
            
            # Assign remaining variables:
            results[,lag:=p]
            results[,num_layer:=layer]
            results[,num_units:=unit]
            results[,dropout:=dropout]
            
            return(results)
          }
        )
      )
      
      return(results)
      
    }
  )
)
saveRDS(grid_search, "results/grid_search.rds")
```

# end

